{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Aim is to create a hybrid movie recommender engine: one that uses both content and collaborative filtering\nto generate recommendations.**\n\n* First we will create a simple recommender that uses weighted rating to create \n  a highest rated and genre wise movie charts.\n\n* Then we will move on to using metadata i.e. using features such as cast, director, genres to generate recommendations\n\n* Following this we will focus on collaborative filtering using Surprise library and combine this with content filtering\n  to create a hybrid movie recommender engine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[](http://)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate\n\nimport warnings; warnings.simplefilter('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# We will first create a Simple Recommender System\n#Load data and examine\nfilepath='/kaggle/input/the-movies-dataset/movies_metadata.csv'\ndf=pd.read_csv(filepath)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since df.genres contains genre info as\n#\"[{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\"\n#It is necessary to extract genres for a movie into a list\n#Note use of fillna, literal_eval and isinstance\ndf['genres']=df['genres'].fillna('[]').apply(literal_eval).apply(lambda x : [d['name'] for d in x] if isinstance(x,list) else [])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weighted Rating (WR) =  ((v/v+m).R)+((m/v+m).C) where,\n# v is the number of votes for the movie\n# m is the minimum votes required to be listed in the chart\n# R is the average rating of the movie\n# C is the mean vote across the whole report\n#Here, we are creating two series vote_counts and vote_averages to calculate C and m\n#We are taking m as 0.95 to create a highest rated movies chart.\nvote_counts=df[df['vote_count'].notnull()]['vote_count'].astype('int')\nvote_averages=df[df['vote_average'].notnull()]['vote_average'].astype('int')\nC = vote_averages.mean()\nm = vote_counts.quantile(0.95)\nprint(C,m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract year from 'release_date'\ndf['year']=pd.to_datetime(df['release_date'],errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a movies chart, with movies above 434 votes only\n#Converting 'vote_count' and 'vote_average' to int to calculate weighted rating\nchart=df[(df['vote_count']>=m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\nchart['vote_count']=chart['vote_count'].astype('int')\nchart['vote_average']=chart['vote_average'].astype('int')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calc. weighted rating and store top 250 movies\ndef weighted_rating(x):\n    v=x['vote_count']\n    R=x['vote_average']\n    return (v/(v+m) * R) + (m/(m+v) * C)\nchart['w_rating']=chart.apply(weighted_rating,axis=1)\nchart=chart.sort_values(by='w_rating',ascending=False).head(250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chart.head(15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a genre wise chart.\n#Take note of stack,level and how gen_df has been created\ns=df.apply(lambda x:pd.Series(x['genres']),axis=1).stack().reset_index(level=1,drop=True)\ns.name = 'genre'\ngen_df = df.drop('genres', axis=1).join(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to build genre-wise chart\ndef build_genre_chart(genre,percentile=0.80):\n    ugenre_df=gen_df[gen_df['genre']==genre]\n    vote_counts=ugenre_df[ugenre_df['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages=ugenre_df[ugenre_df['vote_average'].notnull()]['vote_average'].astype('int')\n    C=vote_averages.mean()\n    m=vote_counts.quantile(percentile)\n    \n    chart=ugenre_df[(ugenre_df['vote_count']>=m) & (ugenre_df['vote_count'].notnull()) & (ugenre_df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]\n    chart['vote_count']=chart['vote_count'].astype('int')\n    chart['vote_average']=chart['vote_average'].astype('int')\n    chart['wr']=chart.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C), axis=1)\n    chart = chart.sort_values('wr', ascending=False).head(250)\n    \n    return chart","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_genre_chart('Romance').head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Content Based Filtering. Will be built on a smaller dataset because computing power is limited.\n#links_small is a smaller data set of around 10000 movies\nlinks_small=pd.read_csv('/kaggle/input/the-movies-dataset/links_small.csv')\nlinks_small=links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\ndf = df.drop([19730, 29503, 35587])\ndf['id']=df['id'].astype('int')\nsdf=df[df['id'].isin(links_small)]\nsdf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a feature called 'description' and apply TfidfVectorizer on it to get feature vectors. These feature vectors \n#will be used to calculate cosine_similarity\nsdf['tagline'] = sdf['tagline'].fillna('')\nsdf['description'] = sdf['overview']+sdf['tagline']\nsdf['description'] = sdf['description'].fillna('')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(sdf['description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cos_sim=linear_kernel(tfidf_matrix, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a series 'indices' with title as index and feature as index from sdf. sdf statement is very important\nsdf=sdf.reset_index()\ntitles = sdf['title']\nindices = pd.Series(sdf.index, index=sdf['title'])\nindices.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendations(title):\n    idx=indices[title]\n    sim_scores=list(enumerate(cos_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores=sim_scores[1:31]\n    movie_indices=[i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather').head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metadata Based Recommender. Adding credits(crew,cast) and keywords for each movie\ncredits = pd.read_csv('/kaggle/input/the-movies-dataset/credits.csv')\nkeywords = pd.read_csv('/kaggle/input/the-movies-dataset/keywords.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge the two dataframes with df and create sdf from links_small\ncredits['id']=credits['id'].astype('int')\nkeywords['id']=keywords['id'].astype('int')\ndf['id']=df['id'].astype('int')\ndf=df.merge(credits,on='id')\ndf=df.merge(keywords,on='id')\nsdf=df[df['id'].isin(links_small)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply literal_eval to return list and create two more features\nsdf['cast']=sdf['cast'].apply(literal_eval)\nsdf['crew']=sdf['crew'].apply(literal_eval)\nsdf['keywords']=sdf['keywords'].apply(literal_eval)\nsdf['crewsize']=sdf['crew'].apply(lambda x:len(x))\nsdf['castsize']=sdf['cast'].apply(lambda x:len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_director(x):\n    for i in x:\n        if i['job']=='Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf['director']=sdf['crew'].apply(get_director)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take the first three actors from each list as they major characters in the movie\nsdf['cast']=sdf['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x,list) else [])\nsdf['cast']=sdf['cast'].apply(lambda x:x[:3] if len(x)>=3 else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract keywords\nsdf['keywords'] = sdf['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf['cast']=sdf['cast'].apply( lambda x: [str.lower(i.replace(\" \",\"\")) for i in x] )\nsdf['director']=sdf['director'].astype('str').apply( lambda x: str.lower(x.replace(\" \", \"\")) )\nsdf['director'] = sdf['director'].apply(lambda x: [x,x, x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sdf.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'keyword'\ns.head()\ns = s.value_counts()\ns[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=s[s>1]\n#Stemming is the process of reducing a word to its word stem\nstemmer=SnowballStemmer('english')\ndef filter_keywords(x):\n    words = []\n    for i in x:\n        if i in s:\n            words.append(i)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer.stem('dogs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf['keywords']=sdf['keywords'].apply(filter_keywords)\nsdf['keywords']=sdf['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\nsdf['keywords'] = sdf['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf['soup'] = sdf['keywords'] + sdf['cast'] + sdf['director'] + sdf['genres']\nsdf['soup'] = sdf['soup'].apply(lambda x: ' '.join(x))\nsdf.soup[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(sdf['soup'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cos_sim=linear_kernel(count_matrix,count_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf = sdf.reset_index()\ntitles = sdf['title']\nindices = pd.Series(sdf.index, index=sdf['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight').head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def improved_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cos_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]\n    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n    C = vote_averages.mean()\n    m = vote_counts.quantile(0.60)\n    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n    qualified['vote_count'] = qualified['vote_count'].astype('int')\n    qualified['vote_average'] = qualified['vote_average'].astype('int')\n    qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n    qualified = qualified.sort_values('wr', ascending=False).head(10)\n    return qualified","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"improved_recommendations('The Dark Knight')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Collaborative filtering\nreader = Reader()\nratings=pd.read_csv('/kaggle/input/the-movies-dataset/ratings_small.csv')\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\nsvd = SVD()\ncross_validate(svd, data, measures=['RMSE', 'MAE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data.build_full_trainset()\nsvd.fit(trainset)\nratings[ratings['userId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings[ratings['userId'] == 1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd.predict(1, 2105,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_int(x):\n    try:\n        return int(x)\n    except:\n        return np.nan\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_map = pd.read_csv('/kaggle/input/the-movies-dataset/links_small.csv')[['movieId', 'tmdbId']]\nid_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)\nid_map.columns = ['movieId', 'id']\nid_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_map = id_map.merge(sdf[['title', 'id']], on='id').set_index('title')\nid_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices_map = id_map.set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hybrid(userId, title):\n    idx = indices[title]\n    tmdbId = id_map.loc[title]['id']\n    #print(idx)\n    movie_id = id_map.loc[title]['movieId']\n    \n    sim_scores = list(enumerate(cos_sim[int(idx)]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]\n    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n    movies = movies.sort_values('est', ascending=False)\n    return movies.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hybrid(500,'Avatar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}