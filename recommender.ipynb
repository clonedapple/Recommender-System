{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim is to create a hybrid movie recommender engine: one that uses both content and collaborative filtering to generate recommendations\n",
    "\n",
    "*First we will create a simple recommender that uses weighted rating to create a highest rated and genre wise movie charts.\n",
    "\n",
    "*Then we will move on to using metadata i.e. using features such as cast, director, genres to generate recommendations\n",
    "\n",
    "*Following this we will focus on collaborative filtering using Surprise library and combine this with content filtering to create a hybrid movie recommender engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# We will first create a Simple Recommender System\n",
    "#Load data and examine\n",
    "filepath='/kaggle/input/the-movies-dataset/movies_metadata.csv'\n",
    "df=pd.read_csv(filepath)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since df.genres contains genre info as\n",
    "#\"[{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\"\n",
    "#It is necessary to extract genres for a movie into a list\n",
    "#Note use of fillna, literal_eval and isinstance\n",
    "df['genres']=df['genres'].fillna('[]').apply(literal_eval).apply(lambda x : [d['name'] for d in x] if isinstance(x,list) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Rating (WR) =  ((v/v+m).R)+((m/v+m).C) where,\n",
    "# v is the number of votes for the movie\n",
    "# m is the minimum votes required to be listed in the chart\n",
    "# R is the average rating of the movie\n",
    "# C is the mean vote across the whole report\n",
    "#Here, we are creating two series vote_counts and vote_averages to calculate C and m\n",
    "#We are taking m as 0.95 to create a highest rated movies chart.\n",
    "vote_counts=df[df['vote_count'].notnull()]['vote_count'].astype('int')\n",
    "vote_averages=df[df['vote_average'].notnull()]['vote_average'].astype('int')\n",
    "C = vote_averages.mean()\n",
    "m = vote_counts.quantile(0.95)\n",
    "print(C,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract year from 'release_date'\n",
    "df['year']=pd.to_datetime(df['release_date'],errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a movies chart, with movies above 434 votes only\n",
    "#Converting 'vote_count' and 'vote_average' to int to calculate weighted rating\n",
    "chart=df[(df['vote_count']>=m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\n",
    "chart['vote_count']=chart['vote_count'].astype('int')\n",
    "chart['vote_average']=chart['vote_average'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc. weighted rating and store top 250 movies\n",
    "def weighted_rating(x):\n",
    "    v=x['vote_count']\n",
    "    R=x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "chart['w_rating']=chart.apply(weighted_rating,axis=1)\n",
    "chart=chart.sort_values(by='w_rating',ascending=False).head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a genre wise chart.\n",
    "#Take note of stack,level and how gen_df has been created\n",
    "s=df.apply(lambda x:pd.Series(x['genres']),axis=1).stack().reset_index(level=1,drop=True)\n",
    "s.name = 'genre'\n",
    "gen_df = df.drop('genres', axis=1).join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build genre-wise chart\n",
    "def build_genre_chart(genre,percentile=0.80):\n",
    "    ugenre_df=gen_df[gen_df['genre']==genre]\n",
    "    vote_counts=ugenre_df[ugenre_df['vote_count'].notnull()]['vote_count'].astype('int')\n",
    "    vote_averages=ugenre_df[ugenre_df['vote_average'].notnull()]['vote_average'].astype('int')\n",
    "    C=vote_averages.mean()\n",
    "    m=vote_counts.quantile(percentile)\n",
    "    \n",
    "    chart=ugenre_df[(ugenre_df['vote_count']>=m) & (ugenre_df['vote_count'].notnull()) & (ugenre_df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]\n",
    "    chart['vote_count']=chart['vote_count'].astype('int')\n",
    "    chart['vote_average']=chart['vote_average'].astype('int')\n",
    "    chart['wr']=chart.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C), axis=1)\n",
    "    chart = chart.sort_values('wr', ascending=False).head(250)\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_genre_chart('Romance').head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Content Based filtering. The recommender above has severe limitations as the recommendations are limited to top movies.\n",
    "\n",
    "-Features such as genre, actors, directors are not taken into consideration when churning out recommendations.\n",
    "\n",
    "-To improve this, content based filtering can be used that computes similarity between movies based on certain metrics and suggests movies that are most similar to a particular movie that a user liked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Content Based Filtering. Will be built on a smaller dataset because computing power is limited.\n",
    "#links_small is a smaller data set of around 10000 movies\n",
    "links_small=pd.read_csv('/kaggle/input/the-movies-dataset/links_small.csv')\n",
    "links_small=links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n",
    "df = df.drop([19730, 29503, 35587])\n",
    "df['id']=df['id'].astype('int')\n",
    "sdf=df[df['id'].isin(links_small)]\n",
    "sdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a feature called 'description' and apply TfidfVectorizer on it to get feature vectors. These feature vectors \n",
    "#will be used to calculate cosine_similarity\n",
    "sdf['tagline'] = sdf['tagline'].fillna('')\n",
    "sdf['description'] = sdf['overview']+sdf['tagline']\n",
    "sdf['description'] = sdf['description'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(sdf['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim=linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a series 'indices' with title as index and feature as index from sdf. sdf statement is very important\n",
    "sdf=sdf.reset_index()\n",
    "titles = sdf['title']\n",
    "indices = pd.Series(sdf.index, index=sdf['title'])\n",
    "indices.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    idx=indices[title]\n",
    "    sim_scores=list(enumerate(cos_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores=sim_scores[1:31]\n",
    "    movie_indices=[i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Godfather').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metadata Based Recommender. Adding credits(crew,cast) and keywords for each movie\n",
    "credits = pd.read_csv('/kaggle/input/the-movies-dataset/credits.csv')\n",
    "keywords = pd.read_csv('/kaggle/input/the-movies-dataset/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two dataframes with df and create sdf from links_small\n",
    "credits['id']=credits['id'].astype('int')\n",
    "keywords['id']=keywords['id'].astype('int')\n",
    "df['id']=df['id'].astype('int')\n",
    "df=df.merge(credits,on='id')\n",
    "df=df.merge(keywords,on='id')\n",
    "sdf=df[df['id'].isin(links_small)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply literal_eval to return list and create two more features\n",
    "sdf['cast']=sdf['cast'].apply(literal_eval)\n",
    "sdf['crew']=sdf['crew'].apply(literal_eval)\n",
    "sdf['keywords']=sdf['keywords'].apply(literal_eval)\n",
    "sdf['crewsize']=sdf['crew'].apply(lambda x:len(x))\n",
    "sdf['castsize']=sdf['cast'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job']=='Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['director']=sdf['crew'].apply(get_director)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the first three actors from each list as they major characters in the movie\n",
    "sdf['cast']=sdf['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x,list) else [])\n",
    "sdf['cast']=sdf['cast'].apply(lambda x:x[:3] if len(x)>=3 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract keywords\n",
    "sdf['keywords'] = sdf['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['cast']=sdf['cast'].apply( lambda x: [str.lower(i.replace(\" \",\"\")) for i in x] )\n",
    "sdf['director']=sdf['director'].astype('str').apply( lambda x: str.lower(x.replace(\" \", \"\")) )\n",
    "sdf['director'] = sdf['director'].apply(lambda x: [x,x, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sdf.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'keyword'\n",
    "s.head()\n",
    "s = s.value_counts()\n",
    "s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=s[s>1]\n",
    "#Stemming is the process of reducing a word to its word stem\n",
    "stemmer=SnowballStemmer('english')\n",
    "def filter_keywords(x):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        if i in s:\n",
    "            words.append(i)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['keywords']=sdf['keywords'].apply(filter_keywords)\n",
    "sdf['keywords']=sdf['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "sdf['keywords'] = sdf['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['soup'] = sdf['keywords'] + sdf['cast'] + sdf['director'] + sdf['genres']\n",
    "sdf['soup'] = sdf['soup'].apply(lambda x: ' '.join(x))\n",
    "sdf.soup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "count_matrix = count.fit_transform(sdf['soup'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim=linear_kernel(count_matrix,count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.reset_index()\n",
    "titles = sdf['title']\n",
    "indices = pd.Series(sdf.index, index=sdf['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Dark Knight').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cos_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:26]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]\n",
    "    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n",
    "    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n",
    "    C = vote_averages.mean()\n",
    "    m = vote_counts.quantile(0.60)\n",
    "    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n",
    "    qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
    "    qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
    "    qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n",
    "    qualified = qualified.sort_values('wr', ascending=False).head(10)\n",
    "    return qualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_recommendations('The Dark Knight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Collaborative filtering\n",
    "-Content based recommenders do not offer recommendations based on the user's personal taste. \n",
    "-Therefore, Collaborative Filtering can be used to offer such a solution.\n",
    "-It is based on the idea that users similar to me can be used to predict how much I will like a particular product or service.\n",
    "\n",
    "-Using the Surprise library that uses extremely powerful algorithms like Singular Value Decomposition (SVD) to minimise RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d914db951c18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Collaborative filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/the-movies-dataset/ratings_small.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movieId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Reader' is not defined"
     ]
    }
   ],
   "source": [
    "#Collaborative filtering\n",
    "reader = Reader()\n",
    "ratings=pd.read_csv('/kaggle/input/the-movies-dataset/ratings_small.csv')\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "svd = SVD()\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "ratings[ratings['userId'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[ratings['userId'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.predict(1, 2105,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommender\n",
    "\n",
    "*This recommender uses both collaborative and content based filtering techniques to provide personalised recommendations to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = pd.read_csv('/kaggle/input/the-movies-dataset/links_small.csv')[['movieId', 'tmdbId']]\n",
    "id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)\n",
    "id_map.columns = ['movieId', 'id']\n",
    "id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = id_map.merge(sdf[['title', 'id']], on='id').set_index('title')\n",
    "id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_map = id_map.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid(userId, title):\n",
    "    idx = indices[title]\n",
    "    tmdbId = id_map.loc[title]['id']\n",
    "    #print(idx)\n",
    "    movie_id = id_map.loc[title]['movieId']\n",
    "    \n",
    "    sim_scores = list(enumerate(cos_sim[int(idx)]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:26]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]\n",
    "    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n",
    "    movies = movies.sort_values('est', ascending=False)\n",
    "    return movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the user ID and movie name(500 and Avatar respectively), suggests movies based on the scores calculated.\n",
    "hybrid(500,'Avatar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
